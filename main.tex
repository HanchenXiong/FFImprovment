%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsmath}
\usepackage{mathtools}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Online Learning]{Online Leanring with Committees and Its Application} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{H. Xiong} % Your name
\institute[online learning group] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Online Learning Group \\ % Your institution for the title page
\medskip
\textit{@hanchenxiong} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section[Online prediction with experts, games, convex optimization]{Online prediction with experts,repeated game playing and convex optimization}        % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
\frame{
    \frametitle{}
    \Huge{Online prediction with experts, repeated game playing and convex optimization}
}

\subsection{Prediction with Experts' advice}
\frame{
    \frametitle{A gentle start} 
    \begin{itemize}
    \item<1->  \structure{Task:}  online prediction of a binary sequence, \emph{i.e.} sequentially forecast a value $y^{(t)}\in\{-1,1\}$ at time $t$ based on historical values $\{y^{(i)}\}_{i=1}^{(t-1}$;
    \item<2->  \structure{Expert committee:} an expert $E_{n}$ is a man/woman who can make a prediction, $f_{n}^{(t)}$, using different strategies (algorithms/heuristics/data resources); assume there are $N$ experts in the committee; 
    \item<3->  \structure{Side information:} other task-relevant information may available, \emph{e.g.} $\{\mathbf{x}^{(t)}\}_{i=1}^{t}$;
    \item<4->  \structure{Forecaster: } an aggregating policy $\pi$ is a function which map experts' advices $\Longrightarrow$ final decision $\hat{y}_t$: $\hat{y}_t = \pi(f_{1}^{(t)}, f_{2}^{(t)}, \cdots, f_{N}^{(t)})$; 
    \item<5->  \structure{Weights update:} at the beginning, each expert is assigned a (uniform) weight $w_{n,0}=1$, and it will be updated along the sequence; 
    \item<6->  \structure{Loss:}  $l(\hat{y}^{(t)}, y^{(t)}) = \mathbf{1}_{\hat{y}^{(t)} \neq y^{(t)}}$, $l(f_{n}^{(t)}, y^{(t)}) = \mathbf{1}_{f_{n}^{(t)} \neq y^{(t)}}$
    \end{itemize}
}


\frame{
    \frametitle{A gentle start, cont.} 

    \onslide<+->{\begin{block}{A simple policy}
    The final decision is made with the \alert{majority voting} from the expert committee: $\hat{y}^{(t)} = \textbf{sign} (\frac{\sum_{n=1}^N f_{i}^{(t)}}{N})$;
    \end{block}}
    

    \onslide<+->{\begin{block}{An ideal scenario}
    we know that there exist some experts who are perfect in the given task;  
    \end{block}}

    \onslide<+->{\begin{block}{A simple weight update scheme}
    $w_{n}^{(t)} \leftarrow 0$ if expert $E_{n}$ makes a mistake at time $t-1$, \emph{i.e.} kick $E_n$ out of the committee; 
    \end{block}}

    \onslide<+->{
    \textbf{Cummulative loss}:
    \begin{equation}
     \hat{L}^{(t)} = \sum_{i=t}^{t}l(\hat{y}^{(t)}, y^{(t)}) \leq \log_{2}N; 
    \end{equation}
    }
}

\frame{
    \frametitle{Generalized committee}
    \onslide<+->{\begin{block}{A more realistic scenario}
     we know that in the committee there exists a best expert who can work better than others in the given task;
    \end{block}}
    \onslide<+->{
    \textbf{Regret $R_{n}^{(t)}$}: the extra losses the forecaster made without exclusively following the expert $E_n$  up to time $t$: 
    \begin{equation}
     R_{n}^{(t)}= \hat{L}^{(t)} - L_{n}^{(t)} = \sum_{i=t}^{t}l(\hat{y}^{(t)}, y^{(t)}) - \sum_{i=t}^{t}l(f_{n}^{(t)}, y^{(t)})
    \end{equation}
    }

    \onslide<+->{\begin{block}{the upper bound of regret}
    $R^{(t)*} = \max_{n\in[1,N]} R_{n}^{(t)} = \hat{L}^{(t)} - \min_{n\in[1,N]}L_{n}^{(t)}$
    \end{block}}
}
    

\frame{
    \frametitle{Weighted majority algorithm}
    \begin{itemize}
    \item  \structure{Task:}  online prediction of a binary sequence, \emph{i.e.} sequentially forecast a value $y^{(t)}\in\{-1,1\}$ at time $t$ based on historical values $\{y^{(i)}\}_{i=1}^{(t-1)}$;
    \item  \structure{Expert committee:} an expert $E_{n}$ is a man/woman who can make a prediction, $f_{n}^{(t)}$, using different strategies (algorithms/heuristics/data resources); assume there are $N$ experts in the committee; 
    \item  \structure{Loss:}  $l(\hat{y}^{(t)}, y^{(t)}) = \mathbf{1}_{\hat{y}^{(t)} \neq y^{(t)}}$, $l(f_{n}^{(t)}, y^{(t)}) = \mathbf{1}_{f_{n}^{(t)} \neq y^{(t)}}$
    \line(1,0){450} 
    \item <2->  \alert{Forecaster: }  $\hat{y}_t = \textbf{sign}(\sum_{n:f_{n,t} =1}w_n^{(t-1)} - \sum_{m:f_{m,t} =-1}w_m^{(t-1)} )$;
    \item <3->  \alert{Weights update:} if one expert $E_n$ predicts wrongly, decrease its weight 
    \begin{equation}
       w_{n}^{(t+1)} = (1-\eta) w_n^{(t)}  
    \end{equation}
    where $\eta \leq \frac{1}{2}$. 
    \end{itemize}   
}


\frame{
    \frametitle{Analysis on weighted majority algorithm}
    \onslide<+->{
    \begin{theorem}[cummulative loss bound using weighted majority]
    After T steps, $\hat{L}^{(T)} \leq 2(1+\eta) \min_{1\in[1,N]} L_{n}^{(T)} + \frac{2\ln N}{\eta}$
    \end{theorem}
    }
    \onslide<+->{
    \alert{Proof:}  Let $\Gamma^{(t)} = \sum_{n \in [1,N]} w_n^{(t)}$, then $\Gamma^{(1)}= N$. Also, if the forecaster makes 
    a mistake, $\hat{y}^{(t)} \neq y^{(t)}$ 
    \begin{equation}
      \Gamma^{(t+1)} \leq \Gamma^{(t)} (\frac{1}{2} + \frac{1}{2} (1-\eta) = \Gamma^{(t)} (1- \frac{\eta}{2})
    \end{equation}
    therefore: 
    \begin{equation}
       \Gamma^{(T+1)} \leq N (1-\frac{\eta}{2})^{\hat{L}^{(T)}} 
    \end{equation}
    for any individual expert $n$
    \begin{equation}
       w_n^{(T+1)}  = (1-\eta)^{L_n^{(T)}}
    \end{equation}
    since $w_n^{(T+1)} \leq \Gamma^{(T+1)} \Longrightarrow(1-\eta)^{L_n^{(T)}} \leq N (1-\frac{\eta}{2})^{\hat{L}^{(T)}} $
    }  
}

\frame{
    \frametitle{Analysis on weighted majority algorithm, cont.}
    \begin{equation}
    \begin{array}{rcl}
                        (1-\eta)^{L_n^{(T)}}    & \leq &  N (1-\frac{\eta}{2})^{\hat{L}^{(T)}}  \\ \\
    \Leftrightarrow         L_n^{(T)} \ln (1-\eta)  & \leq &  \ln N +  \hat{L}^{(T)}  \ln (1-\frac{\eta}{2}) \\ \\
    \Leftrightarrow       -\ln (1-\frac{\eta}{2})   & \leq &  -L_n^{(T)} \ln (1-\eta) + \ln N  \\ \\  
    \xRightarrow{\alert{x\leq -\ln (1-x)}}  \frac{\eta}{2} \hat{L}^{(T)} & \leq & -L_n^{(T)} \ln (1-\eta) + \ln N  \\ \\  
    \xRightarrow{\alert{-\ln (1-x) \leq x + x^2, \text{when } x \leq 1/2 }}  \frac{\eta}{2}  \hat{L}^{(T)} & \leq & L_n^{(T)} \eta(1+\eta) + \ln N  \\ \\  
    \Leftrightarrow  \hat{L}^{(T)} & \leq & 2(1+\eta) L_{n}^{(T)} + \frac{2\ln N}{\eta}
    \end{array}
    \end{equation} 
}

\frame{
    \frametitle{Randomized weighted majority algorithm}
    \begin{itemize}
    \item  \structure{Task:}  online prediction of a binary sequence, \emph{i.e.} sequentially forecast a value $y^{(t)}\in\{-1,1\}$ at time $t$ based on historical values $\{y^{(i)}\}_{i=1}^{(t-1)}$;
    \item  \structure{Expert committee:} an expert $E_{n}$ is a man/woman who can make a prediction, $f_{n}^{(t)}$, using different strategies (algorithms/heuristics/data resources); assume there are $N$ experts in the committee; 
    \item  \structure{Loss:}  $l(\hat{y}^{(t)}, y^{(t)}) = \mathbf{1}_{\hat{y}^{(t)} \neq y^{(t)}}$, $l(f_{n}^{(t)}, y^{(t)}) = \mathbf{1}_{f_{n}^{(t)} \neq y^{(t)}}$
    \item  \structure{Weights update:} if one expert $E_n$ predicts wrongly, decrease its weight 
    \begin{equation}
       w_{n}^{(t+1)} = (1-\eta) w_n^{(t)}  
    \end{equation}
    where $\eta \leq 1/2$. 
    \line(1,0){450} 
    \item <2->  \alert{Forecaster: }  $\hat{y}_t \sim  \textbf{Bernoulli}\left(\frac{\sum_{n:f_{n,t} =1} w_n^{(t-1)}}{\sum_n w_n^{(t-1)}}, \frac{\sum_{n:f_{n,t} =-1} w_n^{(t-1)}}{\sum_n w_n^{(t-1)}}\right)$;
    \end{itemize}   
}

\frame{
    \frametitle{Analysis on randomized weighted majority algorithm}
    \onslide<+->{
    \begin{theorem}[cummulative loss bound using randomized weighted majority]
    After T steps, $\hat{L}^{(T)} \leq (1+\eta) \min_{1\in[1,N]} L_{n}^{(T)} + \frac{\ln N}{\eta}$
    \end{theorem}
    }
    \onslide<+->{
    \alert{Proof:}  Let $\Gamma^{(t)} = \sum_{n \in [1,N]} w_n^{(t)}$, then $\Gamma^{(1)}= N$. 
    \newline 
    At each time $t$, let $F^(t) = \frac{\sum_{n:f_n^{(t)} \neq y^{(t)}} w^{(t)}_n }{\sum_{n} w_n^{(t)}}$, then 
    \begin{equation}
      \Gamma^{(t+1)} = \Gamma^{(t)} \Big( 1-F^{(t)} + F^{(t)}(1-\eta) \Big) = \Gamma^{(t)} (1- F^{(t)}\eta)
    \end{equation}
    therefore: 
    \begin{equation}
       \Gamma^{(T+1)} = N \prod_{t=1}^T(1-F^{(t)}\eta) 
    \end{equation}
    for any individual expert $n:       w_n^{(T+1)}  = (1-\eta)^{L_n^{(T)}}$, \\
    since $w_n^{(T+1)} \leq \Gamma^{(T+1)} \Longrightarrow(1-\eta)^{L_n^{(T)}} \leq N \prod_{t=1}^T(1-F^{(t)}\eta)$
    }  
}


\frame{
    \frametitle{Go beyond binary sequence and 0-1 loss}
    \begin{itemize} 
    \item  \structure{Expert committee:} an expert $E_{n}$ is a man/woman who can make a prediction, $f_{n}^{(t)}$, using different strategies (algorithms/heuristics/data resources); assume there are $N$ experts in the committee; 
    \item  \structure{Forecaster: }  $\hat{y}^{(t)} = \pi(f_{1}^{(t)}, f_{2}^{(t)}, \cdots, f_{N}^{(t)})$; 
    \line(1,0){450} 
    \item  \alert{Task:}  \alert{general online prediction}, \emph{i.e.} sequentially forecast a value $y^{(t)}\in\mathbb{R}$ at time $t$ based on historical values $\{y^{(i)}\}_{i=1}^{(t-1)}$;
    \item  \alert{Loss:}  a cost vector $[m_1^{(t)}, m_2^{(t)}i, \cdots, m_N^{(t)}]$ is incurred to experts' predictions at time $t$;
    \item  \alert{Weights update:}  multiplicative manner
    \begin{equation}
        w_n^{(t+1)} = w_n^{(t)} g(m_n^{(t)})
    \end{equation}
    where $g(x)$ is a decreasing function w.r.t. $x$. 
    \end{itemize}   
}

\frame{
    \frametitle{Revisit aggregation policy in binary case}
     \alert{Forecaster: }  $\hat{y}_t \sim  \textbf{Bernoulli}\left(\frac{\sum_{n:f_{n,t} =1} w_n^{(t-1)}}{\sum_n w_n^{(t-1)}}, \frac{\sum_{n:f_{n,t} =-1} w_n^{(t-1)}}{\sum_n w_n^{(t-1)}}\right)$;
     \newline 
     Let $p^{(t-1)}_n = \frac{w_n^{(t-1)}}{\sum_{n=1}^N w_n^{(t-1)}}$, then 
     \begin{equation}
     \begin{array}{rcl}
        \mathbb{E}\left\{ \hat{y}^{(t)}\right\} & = & (-1)\cdot \sum_{n:f_{n,t}=-1} p_n^{(t-1)}  + 1 \cdot \sum_{n:f_{n,t}=1} p_n^{(t-1)}  \\ \\
                                                & = &  \sum_{n=1} f_n^{(t)} p_n^{(t-1)} \\   \\
                                                & = &  \mathbb{E}_{p^{(t-1)}} \left\{ f^{(t)} \right\}
    \end{array}
     \end{equation}
     A further randomized version:  
     \begin{equation}
         \hat{y}^{(t)} = f_{n_\dagger}^{(t)} \text{ with } n_\dagger \sim [p_1^{(t-1)},  p_2^{(t-1)}, \cdots,p_N^{(t-1)}]
     \end{equation}  
}

\frame{
    \frametitle{Randomized weighted majority algorithm IN GENERAL}
    \begin{itemize} 
    \item  \structure{Expert committee:} an expert $E_{n}$ is a man/woman who can make a prediction, $f_{n}^{(t)}$, using different strategies (algorithms/heuristics/data resources); assume there are $N$ experts in the committee; 
    \line(1,0){450} 
    \item  \alert{Task:}  \alert{general online prediction}, \emph{i.e.} sequentially forecast a value $y^{(t)}\in\mathbb{R}$ at time $t$ based on historical values $\{y^{(i)}\}_{i=1}^{(t-1)}$;
    \item  \alert{Loss:}  a cost vector $[m_1^{(t)}, m_2^{(t)}i, \cdots, m_N^{(t)}]$ is incurred to experts' predictions at time $t$;
    \item  \alert{Forecaster: } $\hat{y}^{(t)} = f_{n_\dagger}^{(t)} \text{ with } n_\dagger \sim [p_1^{(t-1)},  p_2^{(t-1)}, \cdots,p_N^{(t-1)}]$ 
    \item  \alert{Weights update:}  multiplicative manner
    \begin{equation}
        w_n^{(t+1)} = w_n^{(t)}(1-\eta m_n^{(t)})
    \end{equation}
    where $\eta \leq 1/2$
    \end{itemize}   
}

\frame{
    \frametitle{Analysis} 
    \begin{theorem}[Regret bound using randomized weighted majority]
    When $m_n^{(t)} \in [-1, 1], \forall n,t$, after T steps,  \newline
    $\hat{L}^{(T)} \leq \min_{1\in[1,N]} L_{n}^{(T)} + \eta \sum_{t=1}^T |m_n^{(t)}|_1 + \frac{\ln N}{\eta}$
    \end{theorem}
    \alert{Regret bound }: $\mathbf{R}^* \leq \eta T + \frac{\ln N}{\eta}$
}

\frame{
    \frametitle{Hedge algorithm}
    \begin{block}{}
    \alert{Weights update:} 
    \begin{equation}
        w_n^{(t+1)} = w_n^{(t)} \exp(-\eta m_n^{(t)})
    \end{equation}
    where $\eta \in[0,1]$.
    \end{block}
    \begin{theorem}[Regret bound using randomized weighted majority]
    When $m_n^{(t)} \in [-1, 1], \forall n,t$, after T steps,  \newline
    $\hat{L}^{(T)} \leq \min_{1\in[1,N]} L_{n}^{(T)} + \eta \sum_{t=1}^T |m_n^{(t)}|_1 + \frac{\ln N}{\eta}$
    \end{theorem}  
}


\subsection{Online repeated game playing}
\frame{
 \frametitle{One-player game}
 \begin{itemize}
 \item<1-> One player plays a game, at time $t$ he takes one action $a^{(t)} = i\in[1,N]$, then the environment 
 releases the cost for each action $\mathbf{m}^{(t)} = [m_1^{(t)}, m_2^{(t)},\cdots, m_N^{(t)}]^\top, m_n^{(t)}\in[-1,1]$,
 \item<1-> \alert{Note} that the loss function $\mathbf{m}^{(t)}$ can change over time, i.e. the environment is changing.
 \item<2-> if the player selects actions by the probabilities $\mathbf{p}^{(t)}$ computed by randomized weighted majority and update them accordingly, then 
 \begin{equation}
    \sum_{t=1}^T \langle \mathbf{m}^{(t)}, \mathbf{p}^{(t)}\rangle \leq  \sum_{t=1}^T m_i^{(t)} + \eta T + \frac{\ln n}{\eta}
 \end{equation}
 \end{itemize}
}

\frame{
\frametitle{Two-player game} 
\alert{Two-person zero-sum}: two players ($K=2)$ play a game which is defined by a $N\times N$ cost matrix $\mathbf{C}$ ($N$ is 
the number of possible actions for players) , where 
each entry $c_{ij}$ defines \alert{the loss to the row player} when the \structure{row player} takes the action $i\in [1,N]$ and 
the \structure{column player} takes the action $j\in [1,N]$. 


An example cost matrix: 
$\left[\begin{array}{ccccc}
  &   1         &   2         &   3         &   4   \\
1 &   c_{11}    &   c_{12}    &   c_{13}    &   c_{14} \\
2 &   c_{21}    &   c_{22}    &   c_{23}    &   c_{24} \\
3 &   c_{31}    &   c_{32}    &   c_{33}    &   c_{34} \\
4 &   c_{41}    &   c_{42}    &   c_{43}    &   c_{44} 
\end{array}
\right]$

The row player's goal is to minimize its loss while the objective of the column player is to maximize it 

}

\frame{
    \frametitle{Nash Equilibrium}
    \begin{itemize}
      \item<1-> if the row player chooses his action from a distribution $\mathbf{p}$, then the most adversary action the column player should take is 
       \begin{equation}
           j:= \arg\max_{j\in[1,N]}  \mathbb{E}_{i\sim \mathbf{p}} \{c_{ij}\}
       \end{equation}
       \item<2-> similarly, if column player use a distribution $\mathbf{q}$ , then the row player should take:
       \begin{equation}
           i:=  \arg\min_{i\in[1,N]}  \mathbb{E}_{j\sim \mathbf{q}} \{c_{ij}\}
       \end{equation}
       \item<3-> \begin{block}{von Neumann's min-max theorem}
           $\min_{\mathbf{p}}\max_{j}\mathbb{E}_{i\sim \mathbf{p}} \{c_{ij}\} = \max_{\mathbf{q}}\max_{i}\mathbb{E}_{j\sim \mathbf{q}} \{c_{ij}\} = \lambda^*$
                \end{block}
      \item<3-> no player has an incentive of changing his strategy (distribution) if the player does not change his, i.e. every player is happy about 
      current status;  
    \end{itemize}
}


\frame{
    \frametitle{Hanan's algorithm for two-player zero-sum game}
    \onslide<+->\begin{block}{Follow the perturbed leading expert}
     \alert{Forecaster:}  select the action  $i^{(t)} = \arg\min_{i\in [1,N]} \left\{L_i^{(t-1)} + \tau_i \right\}$ 
     where $\tau_i$ are randomly sampled from $[0, 1/\epsilon]$
    \end{block}
    \onslide<+->\begin{block}{Equivalence to exponential weighted majority}
    In Hanan's algorithm, when $\tau_i = \frac{1}{\eta} \ln \ln \frac{1}{u_i}$, where $u_i \sim [0,1]$, 
    \begin{equation*}
        Pr[i^{(t)} = j] =\frac{e^{-\eta L_j^{(t)}}}{\sum_{k=1}^N e^{-\eta L_k^{(t)}}}
    \end{equation*}
    after $T = \frac{4\ln n}{\epsilon}$ iterations, the algorithm can converge to a $\tilde{\mathbf{p}}$ which yield $\lambda^* + \epsilon$
    \end{block}
    
}



\subsection{Online convex optimization}
\frame{
    \frametitle{Arbitrary convex loss function}
    \begin{itemize}
       \item <1-> a convex function family on $\mathbf{p}: g(\mathbf{p})$  
       \item <1-> the whole objective function is presented sequentially: $G(\cdot) = g^{(1)}(\cdot) + g^{(2)}(\cdot) + \cdots  g^{(T)}(\cdot) $ 
       \item <2-> re-design a loss $\mathbf{m}^{(t)} = \frac{1}{\rho} \Delta f^{(t)}(\mathbf{p})$   
       \item <2-> $\sum_{t=1}^T f^{(t)}(\mathbf{p}^{(t)}) - \min_\mathbf{p} G(\mathbf{p}) \leq 2\rho \sqrt{\ln n T}$
       \item <3-> applications \begin{enumerate}
           \item online portfolio management: $g^{(t)}(\mathbf{p}) = \log (-\mathbf{p}^\top \Delta \mathbf{v}^{(t)})$
           \item online learning algorithms: $g^{(t)}(\mathbf{p}) = ||y^{(t)} - \mathbf{p}^\top \mathbf{x}^{(t)}||_2^2 $
       \end{enumerate}
    \end{itemize}
}


%-----------------------------------------------------------------------------------------
\section{Bandit Optimization in metric spaces}
\frame{
    \frametitle{}
    \Huge{Bandit Optimization in metric spaces}
}
\subsection{Bandit: play games with limited feedbacks}
\frame{
    \frametitle{Observe only one cost}  
     \begin{itemize}
        \item  <1-> One player plays a game, at time $t$ he takes one action $a^{(t)} = i\in[1,N]$, then the environment 
            releases the cost for \alert{only the selected action} $m_{a^{(t)}}^{(t)}$, 
        \item  <2-> playing games in a Bandit setting falls into  \alert{classic exploitation v.s. exploration dilemma} 
 \end{itemize}
}


\frame{
    \frametitle{Multi-armed bandit problem}
    \begin{figure}     
        \includegraphics[width = 0.5\textwidth]{bandit-arm.jpg}
    \end{figure}
}

\frame{
    \frametitle{Upper confidence bound}
    \begin{itemize}
     \item <1-> at each time $t$, play the arm $i^{(t)}$ by 
     \begin{equation}
        i^{(t)} = \arg\max_{i\in [1,N]} \left\{\textbf{avg.} [m_i] + \sqrt{\frac{2\ln t}{T_i}} \right\}
     \end{equation}
     \item <2-> Regret bound:
     \begin{equation}
       \mathbf{R}^{(t)*} \leq \left[ 8 \sum_{i: \mu_i < \mu^* }\frac{\ln t}{\mu^* -\mu_i } \right] + (1+\frac{\pi^2}{3}) (\sum_{i=1}^N (\mu^*-\mu_i) )
     \end{equation}
     where $\mu_i$ denotes the true expected reward for arm $i$, and $\mu^*$ denotes the best one.
    \end{itemize}
}


\subsection{Gaussian process bandit optimization}
\frame{
    \frametitle{Contextual bandit optimization}
    \begin{itemize}
       \item <1-> in classic multi-armed bandit problem, all arms are \alert{independent};   
       \item <2-> in many practical applications, \alert{there exists a context beneath arms}, e.g. \structure{representation of arms is in a metric space};    
    \end{itemize}
}

\frame{
    \frametitle{Response surface optimization}
    \begin{itemize}
       \item <1->  response surface optimization is an extension of contextual bandit optimization to infinite number of arms; 
       \item <2->  model the response of arms using a smooth surface function $f$ 
       \item <3-> a demo mimicking the style to understand surface-response optimization; 
       \item <4-> a most general case, \alert{Lipschit  continuity}: if 
       \begin{equation}
            d_x(\mathbf{x}_1, \mathbf{x}_2) \leq L \cdot d_f(f(\mathbf{x}_1), f(\mathbf{x}_2))
       \end{equation}
       then we say $f$ is a $L$-Lipschit continuous function. 
    \end{itemize}
}

\frame{
    \frametitle{Gaussian process}
    \onslide<+->\begin{block}{Definition}
    A Gaussian process (GP) defines is a collection of random variables, any finite number of of which have joint Gaussian distribution.
     \begin{equation}
          f(\mathbf{x}) \sim \mathcal{GP} (\mu(\mathbf{x}), k(\mathbf{x},\mathbf{x}^\prime))
     \end{equation}
    \end{block}
    \onslide<+->\begin{block}{joint Gaussian distribution}
    \begin{equation}
        \left[
        \begin{array}{c}
        f(\mathbf{X}_{train}) \\
        f(\mathbf{x}_{test})
        \end{array}
        \right]
        \sim  \left(
        \left[ 
        \begin{array}{c}
        \mu(\mathbf{X}_{train}) \\
        \mu(\mathbf{x}_{test}) 
        \end{array}
        \right], 
        \left[
           \begin{array}{cc}
             k(X_{train}, X_{train})              , &  k(\mathbf{X}_{train}, \mathbf{x}_{test})  \\
             k(\mathbf{X}_{train}, x_{test})^\top , &  k(\mathbf{x}_{test}, \mathbf{x}_{test})
           \end{array}
        \right]
        \right)
    \end{equation}
    \end{block}
}
\frame{
  \frametitle{Gaussian process, cont.}
    \onslide<+-> \begin{block} {Conditional probability}
    \begin{equation}
         f(\mathbf{x}_{test})| f(\mathbf{X}_{train}) \sim  \mathcal{N} \left(\mu_{pos}(\mathbf{x}_{test}), k_{pos}(\mathbf{x}_{test}, \mathbf{x}^{\prime}) \right)
    \end{equation} 
    \end{block}
   \onslide<+-> \begin{block}{Posterior Gaussian process}
        \begin{equation}
         f(\mathbf{x})|\mathcal{D}_{train} \sim \mathcal{GP} \left(\mu_{pos}(\mathbf{x}), k_{pos}(\mathbf{x}, \mathbf{x}^\prime)\right)
    \end{equation}
    \end{block}
    where  $\mu_{pos}(\mathbf{x}) = \mu(\mathbf{x}) + k(\mathbf{X}_{train}, \mathbf{x})^\top  k(X_{train}, X_{train}) (f(\mathbf{X}_{train}) - \mathbf{X}_{train}) $ and 
    $k_{pos}(\mathbf{x}, \mathbf{x}^\prime) = k(\mathbf{x}, \mathbf{x}^\prime) - k(\mathbf{X}_{train}, \mathbf{x})^\top k(\mathbf{X}_{train}, \mathbf{X}_{train})^{-1} k(\mathbf{X}_{train}, \mathbf{x}^\prime) $.
}


\frame{
    \frametitle{Gaussian process bandit optimization}
    \begin{block}{Kernel function}
       squared exponential kernel:  $k(\mathbf{x}, \mathbf{x}^\prime) = \exp(-\frac{||\mathbf{x} - \mathbf{x}^\prime||}{2l^2})$
    \end{block}
    \begin{block}{Information gain} 
        the informativeness of  a set of points  $A \in \mathcal{X}$ for learning $f$ is defined as:
        \begin{equation}
         I(\mathbf{y}_A; \mathbf{f}_A) = H(\mathbf{y}_A) - H(\mathbf{y}_A|\mathbf{f}_A)
        \end{equation}
    \end{block}
    \begin{block}{Maximum information gain after $T$ iterations} 
      $\gamma^{(T)} = \max_{A\in \mathcal{X}:|A|=T}  I(\mathbf{y}_A; \mathbf{f}_A)$ \newline 
      \begin{itemize}
        \item  maximum information gain basically reflect the complexity of $f$; 
        \item  for RBF kernel, $\gamma^{(T)} = \mathcal{O} \left((\log T)^{d+1}\right)$ 
      \end{itemize}
    \end{block}

}

\frame{
    \frametitle{Upper confidence bound}
    \begin{itemize}
      \item <1-> at each time $t$, chose: 
     \begin{equation}
        \mathbf{x}^{(t)} = \arg\max_{\mathbf{x} \in \mathcal{X}} \mu^{(t-1)}(\mathbf{x}) + \kappa^{(t)} \sigma^{(t-1)}(\mathbf{x})
     \end{equation}
     where $\kappa^{(t)} = 2B + 300 \gamma^{(t)} \log^3(t/\delta)$, $||f||^2 \leq B$ 
     \item <2-> Regret bound:
     \begin{equation}
          Pr\left\{\mathbf{R}^{(T)*} \leq \sqrt{\frac{8}{\log(1+\sigma^{-1})} T \beta^{(T)}\gamma^{(T)}}\right\} \geq 1-\delta
     \end{equation}
     \item <2->  Regret bound complexity: $\mathbf{R}^{(T)*} = \mathcal{O} \left(\sqrt{T(\log T)^{d+1}}\right)$. 
    \end{itemize}
}

\subsection{General Bayesian optimization} 
\frame{
    \frametitle{Regression with ensemble models}
    \begin{itemize} 
       \item <1->  Gaussian process bandit optimization is an instance of Bayesian optimization; 
       \item <1->  any function $f$ with certain \structure{smoothness assumption} and \structure{uncertainty measurement} can be used as a pseudo Bayesian optimization;
       \item <2->  \alert{one example:}  an ensemble of linear functions
       \begin{figure}
        \includegraphics[width = 0.3\textwidth]{lines.jpg}
       \end{figure}
       \begin{enumerate}
        \item \structure{smoothness assumption}: average of linear functions
        \item \structure{uncertainty measurement}: std.of linear functions 
       \end{enumerate}
       \item <2->  advantage: can better exploit task-relevant features instead of \alert{isotropic length-scale} in kernel function 
    \end{itemize}
}

%------------------------------------------------------------------------------------------
\section{Concept drift in online decision}
\frame{
    \frametitle{}
    \Huge{Concept drift in online decision}
}
\frame{
    \frametitle{What do people do when the concept drifts ?}
    \begin{figure}
     \includegraphics[width = 0.7\textwidth]{concept_drift.png}
    \end{figure}
   Assume that the concept drift can be successfully detected, 
    \begin{itemize}
       \item <1-> \structure{forget the data} before the new concept in learning algorithms; 
       \item <2-> \alert{change a new learning algorithm (strategy)}
    \end{itemize}
}
\subsection{Stability v.s. adaptivity}
\frame{
    \frametitle{Stability and adaptivity when forgetting data}
    \begin{block}{Definition} 
        A learning algorithm $\mathcal{A}$ has error stability $\beta_n$ with respect to the loss function $l$ if 
        \begin{equation*}
            \forall Z_n \in \mathcal{X}^n, \forall i\in \{1,\cdots, n\} |\mathbb{E}\{l(\mathcal{A}_{Z_n})\} - \mathbb{E}\{l(\mathcal{A}_{Z_n}^{-i})\} | 
        \end{equation*}
    \end{block}
    \begin{block}{Examples}
        for k-NN, SVM, support vector regression and ridge regression, $\beta_n = \mathcal{O}(\frac{1}{n})$
    \end{block}
    
}
\subsection{Explicit detection of concept drifts}
\frame{
    \frametitle{Detect concept drift}
    \begin{itemize}
        \item Essentially, the detection has been conducted via hypothesis test sequentially. 
        \newline
          $l(\mathcal{A}_{\{\mathbf{x}_1,\cdots, \mathbf{x}_n-t\}}, \underbrace{\{\mathbf{x}_{n-t+1}, \cdots, \mathbf{x}_n\}}_{\text{time window}}) \Longrightarrow$  \newline 
          \alert{null  hypothesis} v.s.\alert{alternative hypothesis}
        \item Can work well with good hyper-parameter settings. 
    \end{itemize}
}
\subsection{Time-varying surface-response bandit optimization}
\frame{
    \frametitle{Temporal-spatial kernel}
    \begin{itemize} 
        \item <1-> Typical analysis of Gaussian process bandit optimization is in a stationary environment.
        \item <2-> When involving the concept drift, the kernel can be defined in a temporal-spatial manner, 
                    i.e. \alert{scale down the kernel values of old instances};
        \item <3-> Regret bound: 
         \begin{equation}
              Pr\left\{\mathbf{R}^{(T)*} \leq \sqrt{\frac{8}{\log(1+\sigma^{-1})} T \beta^{(T)}\alert{\hat{\gamma}^{(T)}}}\right\} \geq 1-\delta
         \end{equation}
    \end{itemize}
}

\subsection{Adaptive regret for tracking the best expert}
\frame{
    \frametitle{Concept drift in expert committee}
    \begin{enumerate} 
        \item \alert{A expert committee} 
        \begin{itemize}
            \item an expert which uses some side information; 
            \item an expert which uses short-memory of instances;
            \item an expert which uses whole sequence of instances; 
            \item an expert which can detect concept drifts and forget old data; 
        \end{itemize}
       
       \item concept drift == best expert shift
    \end{enumerate} 
}

\frame{
    \frametitle{Adaptive regret and Fixed-share algorithm}
    \begin{itemize}          
        \item<1-> regular regret: \newline 
        $R^{(t)*} = \max_{n\in[1,N]} R_{n}^{(t)} = \hat{L}^{(t)} - \min_{n\in[1,N]}L_{n}^{(t)}$
        \item<2->  adaptive regret: \newline 
        $\bar{R}^{(t)*} = \sup_{[t1,t2]\subseteq [1,t]} \left\{ \hat{L}^{[t1,t2]} - \min_{n\in[1,N]}L_{n}^{[t1,t2]} \right\}$
        \item<3-> weight-share algorithm:
        \begin{block}{Fixed-share algorithm}
            \alert{Weight update} 
           if one expert $E_n$ predicts wrongly, decrease its weight 
             \begin{equation}
                \hat{w}_{n}^{(t+1)} = w_n^{(t)}\exp^{(-\eta m_n^{(t)})} 
             \end{equation}
           followed by a weight-share update:
            \begin{equation}
                w_n^{(t+1)} = (1-\alpha) \hat{w}_{n}^{(t+1)}  + \frac{\alpha}{N-1} \sum_{m\neq n}^N \hat{w}_m^{(t+1)} 
            \end{equation}
        \end{block}
    \end{itemize}
}




%------------------------------------------------------------------------------------------
\section{New item proposal plans }
\frame{
    \Huge{New item proposal plans}
}
\subsection{Expert setting v.s. Bandit setting}
\frame{
    \frametitle{Expert committee interpretation}
    by considering each expert is a sku proposer,
    \begin{itemize}
        \item  fully observed feedback for all experts;
        \item  can flexibly define cost function ;
        \item  can flexibly add diverse experts;
        \item  regret bound complexity: $\mathcal{O}(\sqrt{T\ln N})$
        \item  clear understanding of performance in non-stationary environments; 
        \item  can be computational expensive by maintaining many experts;
        \item  there exist some gap between \alert{sequential proposal} and \alert{search} 
    \end{itemize}  
}

\frame{
    \frametitle{Bandit interpretation within ensemble learning}
    \begin{itemize}
       \item partially observed feedback only on the selected sku;
       \item no explicit loss function;
       \item non-trivial analysis of the information gains for different types of functions; 
       \item regret bound complexity: $\mathcal{O} \left(\sqrt{T(\log T)^{d+1}}\right)$ 
       \item difficult to analyze information gain in non-stationary environments;  
       \item GP complexity $\mathcal{O}(n^3)$; also can be computational expensive by maintaining many models;
       \item it stands closer to the nature of the \emph{Search} task;
    \end{itemize}
    
}

\frame{
    \frametitle{Connections}
    if $\kappa = 0$ for all $t$, then \alert{Bandit setting == Expert committee setting} in the aggregation policy:
    \begin{block}{A reminder}
    \begin{equation*}
          \hat{y}^{(t)}  = \sum_{n=1}^{N} p_n^{(t-1)} f_n^{(t)}
    \end{equation*}
    \end{block}
}

\frame{
    \frametitle{Plans and outlooks}
    Under expert committee interpretation framework:
    \begin{itemize} 
      
       \item  <1->define more diverse proposal experts (strategies);
       \item  <1-> design an informative loss function;
       \begin{enumerate}
             \item loss function for classifiers
             \begin{equation*}
                 m_n^{(t)} =  \exp^{- p_n^{(t)} (y^{(t)})}
             \end{equation*}
             where $y^{(n)} = \{-1, 1\}$ and $p_n^{(t)}$ denotes the classification probability of $E_n$ 
             \item loss function for regressor (ranker)
             \begin{equation*}
                    ??                      
            \end{equation*}
       \end{enumerate}
       \item  <2-> enable some experts with concept drift detection and data forgetting mechanism;
       \item  <2-> track the best expert using weight-share algorithm 
    \end{itemize}    
}



\subsection{Algorithm evaluation}
\frame{
    \frametitle{Quantitative evaluation}
    \begin{itemize}
        \item \structure{Expert committee interpretation:} \newline 
        cumulative loss of the forecaster or regret;
        \item \structure{Bandit interpretation: }\newline
        use pseudo targets (wishlist); 
        
    \end{itemize}
}


%------------------------------------------------

\frame{
    \structure{References 1}: Nicolo Cesa-Bianchi and Gabor Lugosi. 2006. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA.
    \structure{References 2}: Online Learning: A Comprehensive Survey 
}
\begin{frame}
\Huge{\centerline{The End}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 
